{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dense_correspondence_manipulation.utils.utils as utils\n",
    "utils.add_dense_correspondence_to_python_path()\n",
    "from dense_correspondence.training.training import *\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "#utils.set_default_cuda_visible_devices()\n",
    "# utils.set_cuda_visible_devices([0]) # use this to manually set CUDA_VISIBLE_DEVICES\n",
    "\n",
    "from dense_correspondence.training.training import DenseCorrespondenceTraining\n",
    "from dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from dense_correspondence.evaluation.evaluation import DenseCorrespondenceEvaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the configuration for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SpartanDataset:\n",
      "   - in train mode\n",
      "   - number of scenes 9\n",
      "   - total images:     3730\n"
     ]
    }
   ],
   "source": [
    "config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', 'dense_correspondence', \n",
    "                               'dataset', 'composite', 'caterpillar_only_9.yaml')\n",
    "config = utils.getDictFromYamlFilename(config_filename)\n",
    "\n",
    "train_config_file = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', 'dense_correspondence', \n",
    "                               'training', 'training.yaml')\n",
    "\n",
    "train_config = utils.getDictFromYamlFilename(train_config_file)\n",
    "dataset = SpartanDataset(config=config)\n",
    "\n",
    "logging_dir = \"code/data_volume/pdc/trained_models/tutorials\"\n",
    "num_iterations = 3500\n",
    "d = 3 # the descriptor dimension\n",
    "name = \"caterpillar_%d\" %(d)\n",
    "train_config[\"training\"][\"logging_dir_name\"] = name\n",
    "train_config[\"training\"][\"logging_dir\"] = logging_dir\n",
    "train_config[\"dense_correspondence_network\"][\"descriptor_dimension\"] = d\n",
    "train_config[\"training\"][\"num_iterations\"] = num_iterations\n",
    "\n",
    "TRAIN = True\n",
    "EVALUATE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "This should take about ~12-15 minutes with a GTX 1080 Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading pose data for scene 2018-04-16-14-25-19\n",
      "INFO:root:Loading pose data for scene 2018-04-16-14-40-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training descriptor of dimension 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading pose data for scene 2018-04-16-14-42-26\n",
      "INFO:root:Loading pose data for scene 2018-04-16-14-44-53\n",
      "INFO:root:Loading pose data for scene 2018-04-16-14-49-22\n",
      "INFO:root:Loading pose data for scene 2018-04-16-15-23-41\n",
      "INFO:root:Loading pose data for scene 2018-04-16-15-25-38\n",
      "INFO:root:Loading pose data for scene 2018-04-16-15-28-45\n",
      "INFO:root:Loading pose data for scene 2018-04-16-15-30-50\n",
      "INFO:root:enabling domain randomization\n",
      "INFO:root:setting up tensorboard_logger\n",
      "INFO:root:tensorboard logger started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using SINGLE_OBJECT_WITHIN_SCENE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py:482: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  own_state[name].copy_(param)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same scene, same object\n",
      "Same scene, same object\n",
      "Same scene, same object\n",
      "Same scene, same object\n",
      "Same scene, same object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:\n",
      "Process Process-4:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 122, in __getitem__\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    return self.get_single_object_within_scene_data()\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 505, in get_single_object_within_scene_data\n",
      "    return self.get_within_scene_data(scene_name, metadata)\n",
      "    img_b_mask=image_b_mask_inv)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 122, in __getitem__\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 653, in get_within_scene_data\n",
      "    return self.get_single_object_within_scene_data()\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 275, in create_non_correspondences\n",
      "    need_to_be_perturbed = where(diffs_1_flattened < threshold, ones, need_to_be_perturbed)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 505, in get_single_object_within_scene_data\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 122, in __getitem__\n",
      "    return self.get_within_scene_data(scene_name, metadata)\n",
      "    return self.get_single_object_within_scene_data()\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 189, in where\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 122, in __getitem__\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 505, in get_single_object_within_scene_data\n",
      "    return self.get_within_scene_data(scene_name, metadata)\n",
      "    return (cond * x_1) + ((1-cond) * x_2)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 653, in get_within_scene_data\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/tensor.py\", line 309, in __mul__\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    img_b_mask=image_b_mask_inv)\n",
      "    return self.get_single_object_within_scene_data()\n",
      "    return self.mul(other)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 122, in __getitem__\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 299, in create_non_correspondences\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 505, in get_single_object_within_scene_data\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 660, in get_within_scene_data\n",
      "    return self.get_single_object_within_scene_data()\n",
      "    image_a_rgb = self.rgb_image_to_tensor(image_a_rgb)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 505, in get_single_object_within_scene_data\n",
      "    uv_b_non_matches_0_flat)\n",
      "    return self.get_within_scene_data(scene_name, metadata)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 1130, in rgb_image_to_tensor\n",
      "    return self.get_within_scene_data(scene_name, metadata)\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 189, in where\n",
      "    return self._rgb_image_to_tensor(img)\n",
      "    return (cond * x_1) + ((1-cond) * x_2)\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 653, in get_within_scene_data\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/tensor.py\", line 309, in __mul__\n",
      "    return self.mul(other)\n",
      "  File \"/home/tpatten/code/pytorch-segmentation-detection/vision/torchvision/transforms.py\", line 34, in __call__\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tpatten/code/dense_correspondence/dataset/spartan_dataset_masked.py\", line 653, in get_within_scene_data\n",
      "    img_b_mask=image_b_mask_inv)\n",
      "    img = t(img)\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 283, in create_non_correspondences\n",
      "    img_b_mask=image_b_mask_inv)\n",
      "    uv_b_non_matches_0_flat = uv_b_non_matches[0].view(-1,1).type(dtype_float).squeeze(1)\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 275, in create_non_correspondences\n",
      "  File \"/home/tpatten/code/pytorch-segmentation-detection/vision/torchvision/transforms.py\", line 81, in __call__\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/_utils.py\", line 38, in _type\n",
      "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
      "    return new_type(self.size()).copy_(self, async)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "    need_to_be_perturbed = where(diffs_1_flattened < threshold, ones, need_to_be_perturbed)\n",
      "  File \"/home/tpatten/code/dense_correspondence/correspondence_tools/correspondence_finder.py\", line 189, in where\n",
      "    return (cond * x_1) + ((1-cond) * x_2)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/tensor.py\", line 303, in __rsub__\n",
      "    return self.new().resize_as_(self).fill_(other).add_(-1, self)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bb8ee266b2b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"training descriptor of dimension %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseCorrespondenceTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"finished training descriptor of dimension %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tpatten/code/dense_correspondence/training/training.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, loss_current_iteration, use_pretrained)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mloss_current_iteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mstart_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/queues.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mracquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mrrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# All of the saved data for this network will be located in the\n",
    "# code/data_volume/pdc/trained_models/tutorials/caterpillar_3 folder\n",
    "\n",
    "if TRAIN:\n",
    "    print \"training descriptor of dimension %d\" %(d)\n",
    "    train = DenseCorrespondenceTraining(dataset=dataset, config=train_config)\n",
    "    train.run()\n",
    "    print \"finished training descriptor of dimension %d\" %(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network quantitatively\n",
    "\n",
    "This should take ~5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(logging_dir, name)\n",
    "model_folder = utils.convert_to_absolute_path(model_folder)\n",
    "\n",
    "if EVALUATE:\n",
    "    DCE = DenseCorrespondenceEvaluation\n",
    "    num_image_pairs = 100\n",
    "    DCE.run_evaluation_on_network(model_folder, num_image_pairs=num_image_pairs)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `evaluation_quantitative_tutorial.ipynb` for a better place to display the plots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
